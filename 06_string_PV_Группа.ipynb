{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа со строковыми значениями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* Макрушин С.В. Лекция \"Работа со строковыми значениям\"\n",
    "* https://pyformat.info/\n",
    "* https://docs.python.org/3/library/re.html\n",
    "    * https://docs.python.org/3/library/re.html#flags\n",
    "    * https://docs.python.org/3/library/re.html#functions\n",
    "* https://pythonru.com/primery/primery-primeneniya-regulyarnyh-vyrazheniy-v-python\n",
    "* https://kanoki.org/2019/11/12/how-to-use-regex-in-pandas/\n",
    "* https://realpython.com/nltk-nlp-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Вывести на экран данные из словаря `obj` построчно в виде `k = v`, задав формат таким образом, чтобы знак равенства оказался на одной и той же позиции во всех строках. Строковые литералы обернуть в кавычки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = {\n",
    "    \"home_page\": \"https://github.com/pypa/sampleproject\",\n",
    "    \"keywords\": \"sample setuptools development\",\n",
    "    \"license\": \"MIT\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"home_page = https://github.com/pypa/sampleproject\"\n",
      "\"keywords  = sample setuptools development\"\n",
      "\"license   = MIT\"\n"
     ]
    }
   ],
   "source": [
    "for elem in obj:\n",
    "    print('\"{:<9} = {}\"'.format(elem, obj[elem]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_page = https://github.com/pypa/sampleproject\n",
      "keywords  = sample setuptools development\n",
      "license   = MIT\n"
     ]
    }
   ],
   "source": [
    "for elem in obj:\n",
    "    print(f\"{elem : <9} = {obj[elem]}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Написать регулярное выражение,которое позволит найти номера групп студентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Евгения гр.ПМ19-1\n",
       "1         Илья пм 20-4\n",
       "2            Анна 20-3\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj2 = pd.Series([\"Евгения гр.ПМ19-1\", \"Илья пм 20-4\", \"Анна 20-3\"])\n",
    "obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-1\n",
      "20-4\n",
      "20-3\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r'([1-2]\\d-\\d)')\n",
    "for line in obj2:\n",
    "    print(p.search(line).group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Разбейте текст формулировки задачи 2 на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Написать',\n",
       " 'регулярное',\n",
       " 'выражение',\n",
       " 'которое',\n",
       " 'позволит',\n",
       " 'найти',\n",
       " 'номера',\n",
       " 'групп',\n",
       " 'студентов',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Написать регулярное выражение, которое позволит найти номера групп студентов.'\n",
    "re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Форматирование строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Загрузите данные из файла `recipes_sample.csv` (__ЛР2__) в виде `pd.DataFrame` `recipes` При помощи форматирования строк выведите информацию об id рецепта и времени выполнения 5 случайных рецептов в виде таблицы следующего вида:\n",
    "\n",
    "    \n",
    "    |      id      |  minutes  |\n",
    "    |--------------------------|\n",
    "    |    61178     |    65     |\n",
    "    |    202352    |    80     |\n",
    "    |    364322    |    150    |\n",
    "    |    26177     |    20     |\n",
    "    |    224785    |    35     |\n",
    "    \n",
    "Обратите внимание, что ширина столбцов заранее неизвестна и должна рассчитываться динамически, в зависимости от тех данных, которые были выбраны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>description</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>44123</td>\n",
       "      <td>90</td>\n",
       "      <td>35193</td>\n",
       "      <td>2002-10-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>67664</td>\n",
       "      <td>10</td>\n",
       "      <td>91970</td>\n",
       "      <td>2003-07-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>38798</td>\n",
       "      <td>30</td>\n",
       "      <td>1533</td>\n",
       "      <td>2002-08-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>these were so go, it surprised even me.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>35173</td>\n",
       "      <td>45</td>\n",
       "      <td>22724</td>\n",
       "      <td>2002-07-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my sister-in-law made these for us at a family...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>84797</td>\n",
       "      <td>25</td>\n",
       "      <td>4470</td>\n",
       "      <td>2004-02-23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>267661</td>\n",
       "      <td>80</td>\n",
       "      <td>200862</td>\n",
       "      <td>2007-11-25</td>\n",
       "      <td>16.0</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>zwetschgenkuchen  bavarian plum cake</td>\n",
       "      <td>386977</td>\n",
       "      <td>240</td>\n",
       "      <td>177443</td>\n",
       "      <td>2009-08-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a traditional fresh plum cake, thought...</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>zwiebelkuchen   southwest german onion cake</td>\n",
       "      <td>103312</td>\n",
       "      <td>75</td>\n",
       "      <td>161745</td>\n",
       "      <td>2004-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a traditional late summer early fall s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>486161</td>\n",
       "      <td>60</td>\n",
       "      <td>227978</td>\n",
       "      <td>2012-08-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a delicious soup that i originally fou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>298512</td>\n",
       "      <td>29</td>\n",
       "      <td>506822</td>\n",
       "      <td>2008-04-15</td>\n",
       "      <td>9.0</td>\n",
       "      <td>i've heard of the 'cookies by design' company,...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name      id  minutes  \\\n",
       "0             george s at the cove  black bean soup   44123       90   \n",
       "1                healthy for them  yogurt popsicles   67664       10   \n",
       "2                      i can t believe it s spinach   38798       30   \n",
       "3                              italian  gut busters   35173       45   \n",
       "4          love is in the air  beef fondue   sauces   84797       25   \n",
       "...                                             ...     ...      ...   \n",
       "29995  zurie s holey rustic olive and cheddar bread  267661       80   \n",
       "29996          zwetschgenkuchen  bavarian plum cake  386977      240   \n",
       "29997   zwiebelkuchen   southwest german onion cake  103312       75   \n",
       "29998                                   zydeco soup  486161       60   \n",
       "29999        cookies by design   cookies on a stick  298512       29   \n",
       "\n",
       "       contributor_id   submitted  n_steps  \\\n",
       "0               35193  2002-10-25      NaN   \n",
       "1               91970  2003-07-26      NaN   \n",
       "2                1533  2002-08-29      NaN   \n",
       "3               22724  2002-07-27      NaN   \n",
       "4                4470  2004-02-23      4.0   \n",
       "...               ...         ...      ...   \n",
       "29995          200862  2007-11-25     16.0   \n",
       "29996          177443  2009-08-24      NaN   \n",
       "29997          161745  2004-11-03      NaN   \n",
       "29998          227978  2012-08-29      NaN   \n",
       "29999          506822  2008-04-15      9.0   \n",
       "\n",
       "                                             description  n_ingredients  \n",
       "0      an original recipe created by chef scott meska...           18.0  \n",
       "1      my children and their friends ask for my homem...            NaN  \n",
       "2                these were so go, it surprised even me.            8.0  \n",
       "3      my sister-in-law made these for us at a family...            NaN  \n",
       "4      i think a fondue is a very romantic casual din...            NaN  \n",
       "...                                                  ...            ...  \n",
       "29995  this is based on a french recipe but i changed...           10.0  \n",
       "29996  this is a traditional fresh plum cake, thought...           11.0  \n",
       "29997  this is a traditional late summer early fall s...            NaN  \n",
       "29998  this is a delicious soup that i originally fou...            NaN  \n",
       "29999  i've heard of the 'cookies by design' company,...           10.0  \n",
       "\n",
       "[30000 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes = pd.read_csv('./data/recipes_sample.csv')\n",
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    id    | minutes  |\n",
      "|---------------------|\n",
      "|  263600  |    30    |\n",
      "|  183051  |    75    |\n",
      "|  471515  |    25    |\n",
      "|  104791  |   1450   |\n",
      "|  170087  |    40    |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnd = np.random.choice(len(recipes), 5, replace=False)\n",
    "print(f'|{\"id\":^10}|{\"minutes\":^10}|')\n",
    "print(f'{\"|\":-<22}|')\n",
    "for i in rnd:\n",
    "    print(f'|{recipes.id[i]:^10}|{recipes.minutes[i]:^10}|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Напишите функцию `show_info`, которая по данным о рецепте создает строку (в смысле объекта python) с описанием следующего вида:\n",
    "\n",
    "```\n",
    "\"Название Из Нескольких Слов\"\n",
    "\n",
    "1. Шаг 1\n",
    "2. Шаг 2\n",
    "----------\n",
    "Автор: contributor_id\n",
    "Среднее время приготовления: minutes минут\n",
    "```\n",
    "\n",
    "    \n",
    "Данные для создания строки получите из файлов `recipes_sample.csv` (__ЛР2__) и `steps_sample.xml` (__ЛР3__). \n",
    "Вызовите данную функцию для рецепта с id `170895` и выведите (через `print`) полученную строку на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_f = open('./data/steps_sample.xml')\n",
    "steps = BeautifulSoup(xml_f, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Leeks And Parsnips  Sauteed Or Creamed\"\n",
      "\n",
      "1. Clean the leeks and discard the dark green portions\n",
      "2. Cut the leeks lengthwise then into one-inch pieces\n",
      "3. Melt the butter in a medium skillet , med\n",
      "4. Heat\n",
      "5. Add the garlic and fry 'til fragrant\n",
      "6. Add leeks and fry until the leeks are tender , about 6-minutes\n",
      "7. Meanwhile , peel and chunk the parsnips into one-inch pieces\n",
      "8. Place in a steaming basket and steam 'til they are as tender as you prefer\n",
      "9. I like them fork-tender\n",
      "10. Drain parsnips and add to the skillet with the leeks\n",
      "11. Add salt and pepper\n",
      "12. Gently sautee together for 5-minutes\n",
      "13. At this point you can serve it , or continue on and cream it:\n",
      "14. In a jar with a screw top , add the half-n-half and arrowroot\n",
      "15. Shake 'til blended\n",
      "16. Turn heat to low under the leeks and parsnips\n",
      "17. Pour in the arrowroot mixture , stirring gently as you pour\n",
      "18. If too thick , gradually add the water\n",
      "19. Let simmer for a couple of minutes\n",
      "20. Taste to adjust seasoning , probably an additional 1 / 2 teaspoon salt\n",
      "21. Serve warm\n",
      "----------\n",
      "Автор: 8377\n",
      "Среднее время приготовления: 27 минут\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_info(name, steps, minutes, author_id):\n",
    "    recipe_steps = '\\n'.join([f'{i}. {step.capitalize()}' for i, step in enumerate(steps, 1)])\n",
    "    return (\n",
    "        f'\"{name.title()}\"\\n\\n'\n",
    "        + recipe_steps +\n",
    "        '\\n----------'\n",
    "        f'\\nАвтор: {author_id}'\n",
    "        f'\\nСреднее время приготовления: {minutes} минут\\n'\n",
    "    )\n",
    "\n",
    "recipe_id = 170895\n",
    "recipe = recipes[recipes.id == recipe_id].iloc[0]\n",
    "recipe_steps = [step.text for step in steps.recipes.find('id', string=recipe_id).parent.find_all('step')]\n",
    "print(show_info(\n",
    "    name=recipe['name'],\n",
    "    steps=recipe_steps,\n",
    "    minutes=recipe.minutes,\n",
    "    author_id=recipe.contributor_id\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    show_info(\n",
    "        name=\"george s at the cove black bean soup\",\n",
    "        steps=[\n",
    "            \"clean the leeks and discard the dark green portions\",\n",
    "            \"cut the leeks lengthwise then into one-inch pieces\",\n",
    "            \"melt the butter in a medium skillet , med\",\n",
    "        ],\n",
    "        minutes=90,\n",
    "        author_id=35193,\n",
    "    )\n",
    "    == '\"George S At The Cove Black Bean Soup\"\\n\\n1. Clean the leeks and discard the dark green portions\\n2. Cut the leeks lengthwise then into one-inch pieces\\n3. Melt the butter in a medium skillet , med\\n----------\\nАвтор: 35193\\nСреднее время приготовления: 90 минут\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с регулярными выражениями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Напишите регулярное выражение, которое ищет следующий паттерн в строке: число (1 цифра или более), затем пробел, затем слова: hour или hours или minute или minutes. Произведите поиск по данному регулярному выражению в каждом шаге рецепта с id 25082. Выведите на экран все непустые результаты, найденные по данному шаблону."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 minutes\n",
      "10 minutes\n",
      "2 hours\n",
      "10 minutes\n",
      "20 minutes\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r'[0-9]{1,}\\s(hours|hour|minutes|minute)')\n",
    "\n",
    "for step in steps.recipes.find('id', string='25082').parent.find_all('step'):\n",
    "    if p.search(step.text):\n",
    "        print(p.search(step.text).group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Напишите регулярное выражение, которое ищет шаблон вида \"this..., but\" _в начале строки_ . Между словом \"this\" и частью \", but\" может находиться произвольное число букв, цифр, знаков подчеркивания и пробелов. Никаких других символов вместо многоточия быть не может. Пробел между запятой и словом \"but\" может присутствовать или отсутствовать.\n",
    "\n",
    "Используя строковые методы `pd.Series`, выясните, для каких рецептов данный шаблон содержится в тексте описания. Выведите на экран количество таких рецептов и 3 примера подходящих описаний (текст описания должен быть виден на экране полностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество рецептов, соответствующих шаблону: 133\n",
      "\n",
      "Примеры описаний:\n",
      "1. this is a great meal eaten the same day ,but even better the next day , if you can wait! add your favourite spices, but try it first as it is and i think that you will enjoy the 'vegetable' taste. good for freezing.\n",
      "2. this was adapted from a recipe i found on the net, but i added julienne onion to the peppers.  this is a meal in itself, or you could have a small slice with a meat dish.  for those that like to have brunch, it's a bit different to your traditional quiche recipes.  if you love cheese, you could add 1/2 cup of your favorite to the egg mixture, then pour over peppers.\n",
      "3. this is kind of similar to some of the other versions out there, but it is the best and easiest i have found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pattern = r'^this[a-zA-Z0-9_ ]*, ?but'\n",
    "\n",
    "def matches_pattern(description):\n",
    "    if isinstance(description, str):\n",
    "        return bool(re.match(pattern, description))\n",
    "    return False\n",
    "\n",
    "matching_recipes = recipes[recipes['description'].apply(matches_pattern)]\n",
    "\n",
    "num_matching_recipes = len(matching_recipes)\n",
    "print(f'Количество рецептов, соответствующих шаблону: {num_matching_recipes}')\n",
    "\n",
    "example_descriptions = matching_recipes['description'].head(3).tolist()\n",
    "print('\\nПримеры описаний:')\n",
    "for i, desc in enumerate(example_descriptions, 1):\n",
    "    print(f'{i}. {desc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. В текстах шагов рецептов обыкновенные дроби имеют вид \"a / b\". Используя регулярные выражения, уберите в тексте шагов рецепта с id 72367 пробелы до и после символа дроби. Выведите на экран шаги этого рецепта после их изменения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces_around_fractions(step):\n",
    "    return re.sub(r'\\s*/\\s*', '/', step)\n",
    "\n",
    "\n",
    "recipe_steps = recipes.loc[recipes['id'] == 72367, 'n_steps']\n",
    "\n",
    "if not recipe_steps.empty:\n",
    "    steps = recipe_steps.iloc[0]\n",
    "    \n",
    "    \n",
    "    if isinstance(steps, str):\n",
    "        steps_list = eval(steps)\n",
    "    elif isinstance(steps, list):\n",
    "        steps_list = steps\n",
    "    else:\n",
    "        steps_list = []\n",
    "\n",
    "        modified_steps = [remove_spaces_around_fractions(step) for step in steps_list]\n",
    "    \n",
    "    \n",
    "    for step in modified_steps:\n",
    "        print(step)\n",
    "else:\n",
    "    print(\"Рецепт с id 72367 не найден.\")\n",
    "    # не понимаю, что не так...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сегментация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Разбейте тексты шагов рецептов на слова при помощи пакета `nltk`. Посчитайте и выведите на экран кол-во уникальных слов среди всех рецептов. Словом называется любая последовательность алфавитных символов (для проверки можно воспользоваться `str.isalpha`). При подсчете количества уникальных слов не учитывайте регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ольга\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'recipes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m recipes_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[43msteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecipes\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      4\u001b[0m     recipes_steps\u001b[38;5;241m.\u001b[39mappend(step\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'recipes'"
     ]
    }
   ],
   "source": [
    "recipes_steps = []\n",
    "\n",
    "for step in steps.recipes.find_all('step'):\n",
    "    recipes_steps.append(step.text)\n",
    "\n",
    "words = set()\n",
    "\n",
    "for step in recipes_steps:\n",
    "    for word in word_tokenize(step):\n",
    "        if word.isalpha():\n",
    "            words.add(word.lower())\n",
    "\n",
    "print(len(words))\n",
    "words\n",
    "#точно также как у вас, но ответ не выходит "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m recipes_steps:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha():\n\u001b[0;32m     11\u001b[0m             words\u001b[38;5;241m.\u001b[39madd(word\u001b[38;5;241m.\u001b[39mlower())\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_pair_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrealign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_contains_sentbreak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_break\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ольга\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "recipes_steps = []\n",
    "\n",
    "for step in recipe_steps:\n",
    "    recipes_steps.append(step)\n",
    "\n",
    "words = set()\n",
    "\n",
    "for step in recipes_steps:\n",
    "    for word in word_tokenize(step):\n",
    "        if word.isalpha():\n",
    "            words.add(word.lower())\n",
    "\n",
    "print(len(words))\n",
    "words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Разбейте описания рецептов из `recipes` на предложения при помощи пакета `nltk`. Найдите 5 самых длинных описаний (по количеству _предложений_) рецептов в датасете и выведите строки фрейма, соответствующие этим рецептами, в порядке убывания длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                        description  \\\n",
      "18408  334113  this wonderful icing is used for icing cakes a...   \n",
      "481    287008  a translucent golden-brown crust allows the gr...   \n",
      "22566  328708  this is one of the best soups i've ever made a...   \n",
      "6779   205348  i wrote this because there are an astounding l...   \n",
      "16296  316000  the first time i made this cake i grated a mil...   \n",
      "\n",
      "       sentence_count  \n",
      "18408              76  \n",
      "481                27  \n",
      "22566              24  \n",
      "6779               23  \n",
      "16296              23  \n"
     ]
    }
   ],
   "source": [
    "def count_sentences(description):\n",
    "    if pd.isna(description):  # Проверка на NaN\n",
    "        return 0\n",
    "    sentences = nltk.sent_tokenize(description)\n",
    "    return len(sentences)\n",
    "\n",
    "recipes['sentence_count'] = recipes['description'].apply(count_sentences)\n",
    "\n",
    "longest_descriptions = recipes.nlargest(5, 'sentence_count')\n",
    "\n",
    "print(longest_descriptions[['id', 'description', 'sentence_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напишите функцию, которая для заданного предложения выводит информацию о частях речи слов, входящих в предложение, в следующем виде:\n",
    "```\n",
    "PRP   VBD   DT      NNS     CC   VBD      NNS        RB   \n",
    " I  omitted the raspberries and added strawberries instead\n",
    "``` \n",
    "Для определения части речи слова можно воспользоваться `nltk.pos_tag`.\n",
    "\n",
    "Проверьте работоспособность функции на названии рецепта с id 241106.\n",
    "\n",
    "Обратите внимание, что часть речи должна находиться ровно посередине над соотвествующим словом, а между самими словами должен быть ровно один пробел.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ольга\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ольга\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Название рецепта с id 241106: eggplant steaks with chickpeas  feta cheese and black olives\n",
      "\n",
      "   JJ     NNS    IN     NNS    VBP    JJ   CC   JJ    NNS  \n",
      "eggplant steaks with chickpeas feta cheese and black olives\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    pos_line = ' '.join(f\"{tag:^{len(word)}}\" for word, tag in pos_tags)\n",
    "    word_line = ' '.join(words)\n",
    "    \n",
    "    print(pos_line)\n",
    "    print(word_line)\n",
    "\n",
    "#id 241106\n",
    "recipe_title = recipes[recipes['id'] == 241106]['name'].values[0]\n",
    "print(f\"Название рецепта с id 241106: {recipe_title}\\n\")\n",
    "pos_tag_sentence(recipe_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
